import os
import time
import math
import requests
from datetime import datetime, timedelta
from urllib.parse import quote
from newspaper import Article, Config
from bs4 import BeautifulSoup
from pymongo import MongoClient, errors
from dotenv import load_dotenv

# â”€â”€â”€ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()
CLIENT_ID     = os.getenv("NAVER_CLIENT_ID")
CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")
MONGO_URI     = os.getenv("MONGO_URI")

if not CLIENT_ID or not CLIENT_SECRET:
    raise RuntimeError("NAVER_CLIENT_ID/SECRET í™˜ê²½ë³€ìˆ˜ í•„ìš”")
if not MONGO_URI:
    raise RuntimeError("MONGO_URI í™˜ê²½ë³€ìˆ˜ í•„ìš”")

# â”€â”€â”€ MongoDB ì—°ê²° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
client = MongoClient(MONGO_URI)
db     = client["newsdb"]
col    = db.articles
col.create_index("url", unique=True)

# â”€â”€â”€ í‚¤ì›Œë“œ ëª©ë¡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
KEYWORDS = [
    "LG", "KOSPI", "KOSDAQ"
]

# KEYWORDS = ["ê¸ˆë¦¬", "ì™„í™”","í™˜ìœ¨", "ë¬¼ê°€", "CPI", "GDP","ì—°ì¤€", "ì¼ë³¸ì€í–‰","ì½”ìŠ¤í”¼", "ì½”ìŠ¤ë‹¥", "ë°˜ë„ì²´", "AI", "ì œì•½","ë°”ì´ì˜¤","ì‚¼ì„±ì „ì", "SKí•˜ì´ë‹‰ìŠ¤", "í˜„ëŒ€ì°¨", "í•œí™”","ì „ìŸ", "ë¬´ì—­ì „ìŸ", "ê¸ˆìœµ", "ê²½ì œ"]

# â”€â”€â”€ newspaper3k ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
config = Config()
config.request_timeout = 20

# â”€â”€â”€ ìˆ˜ì§‘ ê¸°ê°„ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
START_DATE = datetime(2023, 1, 1)
END_DATE   = datetime(2024, 12, 31)

# â”€â”€â”€ API í˜¸ì¶œ ì œí•œ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MAX_REQUESTS   = 25000
request_count  = 0
limit_exceeded = False

# â”€â”€â”€ í˜ì´ì§• & ìŠ¬ë¼ì´ìŠ¤ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PER_PAGE   = 100   # í•œ í˜ì´ì§€ë‹¹ 100ê±´
PAGES      = 10    # start=1,101,â€¦,901 â†’ 10í˜ì´ì§€ â†’ ìµœëŒ€ 1,000ê±´
SLICES     = 10    # ê¸°ê°„ì„ 10ì¡°ê°ìœ¼ë¡œ ë¶„í•  â†’ ì´ 10Ã—1,000ê±´ = 10,000ê±´

slice_days = math.ceil((END_DATE - START_DATE).days / SLICES)

def fetch_meta_page(query: str, start: int, display: int = PER_PAGE):
    """ë„¤ì´ë²„ ë‰´ìŠ¤ ë©”íƒ€ ìˆ˜ì§‘(í˜ì´ì§€ ë‹¨ìœ„)"""
    global request_count, limit_exceeded
    if request_count >= MAX_REQUESTS:
        limit_exceeded = True
        return []
    url = (
        "https://openapi.naver.com/v1/search/news.json"
        f"?query={quote(query)}&display={display}&start={start}&sort=date"
    )
    headers = {
        "X-Naver-Client-Id":     CLIENT_ID,
        "X-Naver-Client-Secret": CLIENT_SECRET,
    }
    try:
        resp = requests.get(url, headers=headers, timeout=10)
        if resp.status_code == 429:
            limit_exceeded = True
            print("ğŸ›‘ HTTP 429: API í•œë„ ì´ˆê³¼")
            return []
        resp.raise_for_status()
        request_count += 1
        return resp.json().get("items", [])
    except requests.exceptions.RequestException as e:
        print("âš ï¸ fetch_meta_page ì˜¤ë¥˜:", e)
        return []

def fetch_headline(url: str, retries: int = 2):
    """1) newspaper3kë¡œ <title> ì‹œë„
       2) ì´ìƒí•˜ë©´ BeautifulSoup <h1> fallback"""
    for _ in range(retries):
        try:
            art = Article(url, language="ko", config=config)
            art.download(); art.parse()
            title = art.title.strip()
            # ì‚¬ì´íŠ¸ëª…ë§Œ ë‚˜ì˜¬ ê²½ìš° ì˜ˆì™¸ ì²˜ë¦¬
            if len(title) > 5 and "Capital Markets" not in title:
                return title
        except Exception:
            pass
        time.sleep(1)
    # fallback: <h1> íƒœê·¸ ì¶”ì¶œ
    try:
        resp = requests.get(url, timeout=10)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "lxml")
        h1 = soup.find("h1")
        if h1 and h1.get_text(strip=True):
            return h1.get_text(strip=True)
    except Exception:
        pass
    return None

def save_to_mongo(item, headline, kw, adjusted_date):
    url = item.get("originallink") or item.get("link")
    doc = {
        "source":       kw,
        "url":          url,
        "pubDate":      item.get("pubDate"),
        "adjustedDate": adjusted_date.isoformat(),
        "headline":     headline,
        "fetchedAt":    datetime.utcnow().isoformat()
    }
    try:
        col.insert_one(doc)
        return True
    except errors.DuplicateKeyError:
        return False

def run_news_collector():
    global request_count, limit_exceeded
    total_saved = 0

    # 1) ë‚ ì§œ ë²”ìœ„ë¥¼ SLICESê°œë¡œ ìª¼ê°­ë‹ˆë‹¤
    slices = []
    for i in range(SLICES):
        s = START_DATE + timedelta(days=i*slice_days)
        e = s + timedelta(days=slice_days-1)
        if e > END_DATE: e = END_DATE
        slices.append((s.date(), e.date()))

    # 2) ê° í‚¤ì›Œë“œë§ˆë‹¤ 10,000ê±´ ì‹œë„
    for kw in KEYWORDS:
        if limit_exceeded: break
        print(f"\nâ–¶ í‚¤ì›Œë“œ '{kw}' ìµœëŒ€ 10,000ê±´ ìˆ˜ì§‘ ì‹œì‘")
        saved = 0
        for (sd, ed) in slices:
            if limit_exceeded: break
            date_filter = f"{sd}..{ed}"
            query = f"{kw} {date_filter}"
            for p in range(PAGES):
                if limit_exceeded: break
                start_idx = p * PER_PAGE + 1
                items = fetch_meta_page(query, start_idx)
                print(f"[{kw}] {sd}~{ed} í˜ì´ì§€ {p+1}/{PAGES} â†’ {len(items)}ê±´")
                for item in items:
                    try:
                        pub_dt = datetime.strptime(
                            item["pubDate"], "%a, %d %b %Y %H:%M:%S %z"
                        )
                    except Exception:
                        continue
                    market_close = pub_dt.replace(hour=15, minute=30, second=0)
                    adjusted = (pub_dt + timedelta(days=1)).date() \
                                if pub_dt > market_close else pub_dt.date()
                    hl = fetch_headline(item.get("link"))
                    if hl and save_to_mongo(item, hl, kw, adjusted):
                        saved += 1
                        total_saved += 1
                time.sleep(1)
        print(f"âœ” í‚¤ì›Œë“œ '{kw}' ì €ì¥ ì™„ë£Œ: {saved}ê±´")

    print(f"\nâœ… ì „ì²´ ì €ì¥ëœ ë‰´ìŠ¤: {total_saved}ê±´  (ìš”ì²­ íšŸìˆ˜: {request_count})")

if __name__ == "__main__":
    run_news_collector()